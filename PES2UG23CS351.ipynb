{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d37c58-dc69-4c0d-a78d-6cf42ec116c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8ec03e34-4d6c-4fd0-98f7-026ed2278984)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
      "\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long\n",
      "\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model_name, framework=\"pt\")\n",
    "        output = generator(prompt, max_length=30, num_return_sequences=1)\n",
    "        print(output[0][\"generated_text\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70bb33b3-afc3-4c18-9bf7-bae94daa557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is cam cam cam spin cam cam whetherclip cam cam endogenousEFFrily cam camue firesmania cam Patrimania cam spinmania spinmaniamaniamaniaEFF\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bart_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"facebook/bart-base\",\n",
    "    framework=\"pt\",\n",
    "    device=-1   # Force CPU to avoid MPS bug\n",
    ")\n",
    "\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "output = bart_generator(prompt, max_new_tokens=30)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba14eb0f-167b-4f65-bc44-72226a092e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "##In this cell, I forced BART to run on the CPU instead of the MPS backend to avoid the device-specific error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d78daad-271e-4fdf-b23c-a7e84b9a21cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b2739bca-abd3-476c-96ae-84221d447ae0)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the goal of generative ai is to create new content.  | score: 0.54\n",
      "the goal of generative ai is to generate new content.  | score: 0.156\n",
      "the goal of generative ai is to produce new content.  | score: 0.054\n",
      "\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No mask_token (<mask>) found on the input\n",
      "\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No mask_token (<mask>) found on the input\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    try:\n",
    "        fill = pipeline(\"fill-mask\", model=model_name,framework=\"pt\")\n",
    "        outputs = fill(sentence)\n",
    "        for o in outputs[:3]:\n",
    "            print(o[\"sequence\"], \" | score:\", round(o[\"score\"], 3))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74e6a4e-8836-4ee3-b962-207fd4e9e19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No mask_token ([MASK]) found on the input\n",
      "\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of Generative AI is to generate new content.  | score: 0.371\n",
      "The goal of Generative AI is to create new content.  | score: 0.368\n",
      "The goal of Generative AI is to discover new content.  | score: 0.084\n",
      "\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of Generative AI is to create new content.  | score: 0.075\n",
      "The goal of Generative AI is to help new content.  | score: 0.066\n",
      "The goal of Generative AI is to provide new content.  | score: 0.061\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    try:\n",
    "        fill = pipeline(\"fill-mask\", model=model_name,framework=\"pt\")\n",
    "        outputs = fill(sentence)\n",
    "        for o in outputs[:3]:\n",
    "            print(o[\"sequence\"], \" | score:\", round(o[\"score\"], 3))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8902c18-2511-423e-b679-3c9bb7841289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.011879235040396452, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n",
      "\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.004000829067081213, 'start': 32, 'end': 60, 'answer': 'risks such as hallucinations'}\n",
      "\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.025211719796061516, 'start': 20, 'end': 61, 'answer': 'significant risks such as hallucinations,'}\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model_name,framework=\"pt\")\n",
    "        answer = qa(question=question, context=context)\n",
    "        print(answer)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f79f1-86ea-4679-a1d7-5190a4ca84b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
